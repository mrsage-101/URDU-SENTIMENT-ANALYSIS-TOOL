{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84853cf0-cfce-474a-a1ad-e0e0d4f0bcf2",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a231fe6-a10c-43ef-87ce-2b09bfc396a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a970afb-b73d-4f75-a18e-a29094e7e56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1aeb7e-99a6-4614-a225-34484dac96e4",
   "metadata": {},
   "source": [
    "#### load datset from uploaded file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0f85b9c-3716-4cdd-a112-5fdcc2d2679d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urdu_text</th>\n",
       "      <th>is_sarcastic</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ğŸ¤£ğŸ˜‚ğŸ˜‚ ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ÛÛ’ Ú©ÙˆØ¬ÛŒ Ù†Û...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ù…ÛŒÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ù†ÛÛŒÚº Ù¾Ø§Ø¦ÛŒÙ† ğŸ˜</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>`` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ú©Û’ Ø¨Ú¾ÛŒØ³ Ù…ÛŒÚº ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           urdu_text  is_sarcastic  \\\n",
       "0  ğŸ¤£ğŸ˜‚ğŸ˜‚ ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ÛÛ’ Ú©ÙˆØ¬ÛŒ Ù†Û...           1.0   \n",
       "1  Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ù…ÛŒÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ...           1.0   \n",
       "2  Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...           0.0   \n",
       "3                                       Ù†ÛÛŒÚº Ù¾Ø§Ø¦ÛŒÙ† ğŸ˜           0.0   \n",
       "4   `` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ú©Û’ Ø¨Ú¾ÛŒØ³ Ù…ÛŒÚº ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ...           1.0   \n",
       "\n",
       "   Unnamed: 2  Unnamed: 3  Unnamed: 4  Unnamed: 5 Unnamed: 6  Unnamed: 7  \n",
       "0         NaN         NaN         NaN         NaN        NaN         NaN  \n",
       "1         NaN         NaN         NaN         NaN        NaN         NaN  \n",
       "2         NaN         NaN         NaN         NaN        NaN         NaN  \n",
       "3         NaN         NaN         NaN         NaN        NaN         NaN  \n",
       "4         NaN         NaN         NaN         NaN        NaN         NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'urdu_sarcastic_dataset.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c79d885-c433-46f3-b346-f00e9591892d",
   "metadata": {},
   "source": [
    "## Cleaning the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a447131-dfea-4aa1-ad04-93267c9bc187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20060 entries, 0 to 20059\n",
      "Data columns (total 8 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   urdu_text     19955 non-null  object \n",
      " 1   is_sarcastic  20004 non-null  float64\n",
      " 2   Unnamed: 2    0 non-null      float64\n",
      " 3   Unnamed: 3    0 non-null      float64\n",
      " 4   Unnamed: 4    0 non-null      float64\n",
      " 5   Unnamed: 5    0 non-null      float64\n",
      " 6   Unnamed: 6    17 non-null     object \n",
      " 7   Unnamed: 7    38 non-null     float64\n",
      "dtypes: float64(6), object(2)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5f77d9-f6a8-456d-94f4-31d1d718eaa4",
   "metadata": {},
   "source": [
    "#### removing the unamed: 2,3,4,5,6,7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a4a617c-61c8-4a0d-9b3f-d4b410b849e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20060 entries, 0 to 20059\n",
      "Data columns (total 2 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   urdu_text     19955 non-null  object \n",
      " 1   is_sarcastic  20004 non-null  float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 313.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df_cleansed = df[['urdu_text', 'is_sarcastic']]\n",
    "df_cleansed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54998065-bce3-41ec-a18f-4f7a3ca9aaa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urdu_text</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ğŸ¤£ğŸ˜‚ğŸ˜‚ ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ÛÛ’ Ú©ÙˆØ¬ÛŒ Ù†Û...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ù…ÛŒÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ù†ÛÛŒÚº Ù¾Ø§Ø¦ÛŒÙ† ğŸ˜</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>`` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ú©Û’ Ø¨Ú¾ÛŒØ³ Ù…ÛŒÚº ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           urdu_text  is_sarcastic\n",
       "0  ğŸ¤£ğŸ˜‚ğŸ˜‚ ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ÛÛ’ Ú©ÙˆØ¬ÛŒ Ù†Û...           1.0\n",
       "1  Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ù…ÛŒÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ...           1.0\n",
       "2  Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...           0.0\n",
       "3                                       Ù†ÛÛŒÚº Ù¾Ø§Ø¦ÛŒÙ† ğŸ˜           0.0\n",
       "4   `` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ú©Û’ Ø¨Ú¾ÛŒØ³ Ù…ÛŒÚº ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ...           1.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleansed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ccfcd52d-3dc8-4a78-ab53-57f356612e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20004.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.500050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.500012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       is_sarcastic\n",
       "count  20004.000000\n",
       "mean       0.500050\n",
       "std        0.500012\n",
       "min        0.000000\n",
       "25%        0.000000\n",
       "50%        1.000000\n",
       "75%        1.000000\n",
       "max        1.000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleansed.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8229b28a-c9b3-411a-b853-116c441f687f",
   "metadata": {},
   "source": [
    "#### here you can see total rows are 20060\n",
    "#### urdu_text has 19955 out of 20060, they should be taken care of \n",
    "#### same for the is_sarcastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc38eb3e-e2bf-4c56-926b-57019517a00b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "urdu_text       105\n",
       "is_sarcastic     56\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleansed.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17c4ea97-32ac-46f3-9735-d9049254e2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20060 entries, 0 to 20059\n",
      "Data columns (total 2 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   urdu_text     19955 non-null  object \n",
      " 1   is_sarcastic  20004 non-null  float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 313.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# now we have the count we will remove them\n",
    "def_cleansed = df_cleansed.dropna(subset=['urdu_text', 'is_sarcastic'])\n",
    "df_cleansed.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0377ae-5880-43f8-9a8e-71c368610686",
   "metadata": {},
   "source": [
    "## Step: 1 Text Processing For Urdu\n",
    "### Removal of Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99810992-bb65-4a39-a2aa-b4e8b1660d39",
   "metadata": {},
   "source": [
    "#### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69fea745-aba7-4691-899c-54c2806442fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c8173bd-8370-4b65-bf09-2323d5de0280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ú©Û’',\n",
       " 'Ú©Ø§',\n",
       " 'Ú©ÛŒ',\n",
       " 'ÛÛ’',\n",
       " 'ÛÛŒÚº',\n",
       " 'Ú©Ùˆ',\n",
       " 'Ù…ÛŒÚº',\n",
       " 'Ø³Û’',\n",
       " 'Ø§ÙˆØ±',\n",
       " 'Ù¾Ø±',\n",
       " 'ØªÚ¾Ø§',\n",
       " 'ØªÚ¾ÛŒ',\n",
       " 'ØªÚ¾Û’',\n",
       " 'ÛŒÛ',\n",
       " 'ÙˆÛ',\n",
       " 'Ø§ÛŒÚ©',\n",
       " 'Ú©Ú†Ú¾',\n",
       " 'Ù„ÛŒÛ’',\n",
       " 'Ø¬Ø³',\n",
       " 'Ø¬Ù†']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urdu_stopwords = [\n",
    "    \"Ú©Û’\", \"Ú©Ø§\", \"Ú©ÛŒ\", \"ÛÛ’\", \"ÛÛŒÚº\", \"Ú©Ùˆ\", \"Ù…ÛŒÚº\", \"Ø³Û’\", \"Ø§ÙˆØ±\", \"Ù¾Ø±\", \"ØªÚ¾Ø§\", \"ØªÚ¾ÛŒ\", \n",
    "    \"ØªÚ¾Û’\", \"ÛŒÛ\", \"ÙˆÛ\", \"Ø§ÛŒÚ©\", \"Ú©Ú†Ú¾\", \"Ù„ÛŒÛ’\", \"Ø¬Ø³\", \"Ø¬Ù†\", \"Ø¬Ùˆ\", \"Ú©Û\", \"Ù†Û’\", \"Ø¨Ú¾ÛŒ\",\n",
    "    \"Ú©Ø±\",\"Ø§Ú¯Ø±\", \"ØªÙˆ\", \"Ø¬Ø¨\", \"ØªÚ©\", \"ØªØ§Ú©Û\", \"Ø¬ÛŒØ³Ø§\", \"Ø¬ÛŒØ³Û’\", \"Ø¬Ø³\", \"Ø¨ØºÛŒØ±\", \"ÛÛ’\", \"Ù†ÛÛŒÚº\",\n",
    "    \"Ú¾Û’\",\"Ø¨Ú¾ÛŒ\",\"Ú©Ø±\",\"Ø¢Ù¾\",\"Ø§Ø³\",\"ÙˆÛ\",\"Ù†Û\",\"Ø§Ø¨\",\"Ø¬Ùˆ\",\"Ø¬ÛŒ\",\n",
    "]\n",
    "urdu_stopwords[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503f85a9-289d-457c-89ef-9752af5acf2f",
   "metadata": {},
   "source": [
    "#### implementing the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42a7cd29-3a49-41ee-a24e-30eaa921fa61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urdu_text</th>\n",
       "      <th>urdu_text_no_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ğŸ¤£ğŸ˜‚ğŸ˜‚ ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ÛÛ’ Ú©ÙˆØ¬ÛŒ Ù†Û...</td>\n",
       "      <td>ğŸ¤£ğŸ˜‚ğŸ˜‚ ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© Ú©ÙˆØ¬ÛŒ Ú†Ø§ÛÛŒÛ’...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ù…ÛŒÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ...</td>\n",
       "      <td>Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ Ø¢Úº Ù…ÛŒÚºğŸ˜‚ğŸ˜‚</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...</td>\n",
       "      <td>Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ù†ÛÛŒÚº Ù¾Ø§Ø¦ÛŒÙ† ğŸ˜</td>\n",
       "      <td>Ù¾Ø§Ø¦ÛŒÙ† ğŸ˜</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>`` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ú©Û’ Ø¨Ú¾ÛŒØ³ Ù…ÛŒÚº ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ...</td>\n",
       "      <td>`` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ø¨Ú¾ÛŒØ³ ÚˆÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ '' Ø­Ø§Ù…Ø¯ Ù…ÛŒØ±ğŸ˜</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           urdu_text  \\\n",
       "0  ğŸ¤£ğŸ˜‚ğŸ˜‚ ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ÛÛ’ Ú©ÙˆØ¬ÛŒ Ù†Û...   \n",
       "1  Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ù…ÛŒÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ...   \n",
       "2  Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...   \n",
       "3                                       Ù†ÛÛŒÚº Ù¾Ø§Ø¦ÛŒÙ† ğŸ˜   \n",
       "4   `` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ú©Û’ Ø¨Ú¾ÛŒØ³ Ù…ÛŒÚº ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ...   \n",
       "\n",
       "                              urdu_text_no_stopwords  \n",
       "0  ğŸ¤£ğŸ˜‚ğŸ˜‚ ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© Ú©ÙˆØ¬ÛŒ Ú†Ø§ÛÛŒÛ’...  \n",
       "1   Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ Ø¢Úº Ù…ÛŒÚºğŸ˜‚ğŸ˜‚  \n",
       "2  Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...  \n",
       "3                                            Ù¾Ø§Ø¦ÛŒÙ† ğŸ˜  \n",
       "4   `` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ø¨Ú¾ÛŒØ³ ÚˆÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ '' Ø­Ø§Ù…Ø¯ Ù…ÛŒØ±ğŸ˜  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.SettingWithCopyWarning)\n",
    "\n",
    "def remove_stopwords(text, stopwords):\n",
    "    # Check if text is a string; if not, return an empty string\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    \n",
    "    # Tokenize the text by splitting on spaces\n",
    "    words = text.split()\n",
    "    # Initialize an empty list to store the filtered words\n",
    "    filtered_words = []\n",
    "    # Simple loop to remove words that are in the stopwords list\n",
    "    for word in words:\n",
    "        if word not in stopwords:\n",
    "            filtered_words.append(word)\n",
    "    # Rejoin the filtered words back into a string\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Applying the stopword removal function to 'urdu_text' column\n",
    "df_cleansed.loc[:, 'urdu_text_no_stopwords'] = df_cleansed['urdu_text'].apply(lambda x: remove_stopwords(x, urdu_stopwords))\n",
    "\n",
    "# Displaying both before and after stopword removal\n",
    "df_cleansed[['urdu_text', 'urdu_text_no_stopwords']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf7bb78-d235-4493-80ea-5e8a3f4a7901",
   "metadata": {},
   "source": [
    "## Removal of emoji, hashtag, URL, Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a502e598-00ef-45f6-8447-4ee768c9b11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emojiNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Using cached emoji-2.13.2-py3-none-any.whl.metadata (5.8 kB)\n",
      "Using cached emoji-2.13.2-py3-none-any.whl (553 kB)\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-2.13.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07384f77-9583-4603-b50d-6da704a1b81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c363aae-12ff-4b9c-8000-2f8984886ac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urdu_text_no_stopwords</th>\n",
       "      <th>urdu_text_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ğŸ¤£ğŸ˜‚ğŸ˜‚ ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© Ú©ÙˆØ¬ÛŒ Ú†Ø§ÛÛŒÛ’...</td>\n",
       "      <td>Ù…Ø«Ø¨ØªÙ…Ø«Ø¨ØªÙ…Ø«Ø¨Øª ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© Ú©...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ Ø¢Úº Ù…ÛŒÚºğŸ˜‚ğŸ˜‚</td>\n",
       "      <td>Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ Ø¢Úº Ù…ÛŒÚº...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...</td>\n",
       "      <td>Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ù¾Ø§Ø¦ÛŒÙ† ğŸ˜</td>\n",
       "      <td>Ù¾Ø§Ø¦ÛŒÙ† Ù…Ø«Ø¨Øª</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>`` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ø¨Ú¾ÛŒØ³ ÚˆÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ '' Ø­Ø§Ù…Ø¯ Ù…ÛŒØ±ğŸ˜</td>\n",
       "      <td>`` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ø¨Ú¾ÛŒØ³ ÚˆÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ '' Ø­Ø§Ù…Ø¯ Ù…ÛŒØ±</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              urdu_text_no_stopwords  \\\n",
       "0  ğŸ¤£ğŸ˜‚ğŸ˜‚ ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© Ú©ÙˆØ¬ÛŒ Ú†Ø§ÛÛŒÛ’...   \n",
       "1   Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ Ø¢Úº Ù…ÛŒÚºğŸ˜‚ğŸ˜‚   \n",
       "2  Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...   \n",
       "3                                            Ù¾Ø§Ø¦ÛŒÙ† ğŸ˜   \n",
       "4   `` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ø¨Ú¾ÛŒØ³ ÚˆÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ '' Ø­Ø§Ù…Ø¯ Ù…ÛŒØ±ğŸ˜   \n",
       "\n",
       "                                   urdu_text_cleaned  \n",
       "0  Ù…Ø«Ø¨ØªÙ…Ø«Ø¨ØªÙ…Ø«Ø¨Øª ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© Ú©...  \n",
       "1  Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ Ø¢Úº Ù…ÛŒÚº...  \n",
       "2  Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...  \n",
       "3                                         Ù¾Ø§Ø¦ÛŒÙ† Ù…Ø«Ø¨Øª  \n",
       "4    `` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ø¨Ú¾ÛŒØ³ ÚˆÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ '' Ø­Ø§Ù…Ø¯ Ù…ÛŒØ±  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dictionary to map some common emojis to their sentiment words in Urdu\n",
    "\n",
    "emoji_sentiment_dict = {\n",
    "    \"ğŸ˜‚\": \"Ù…Ø«Ø¨Øª\",  # positive\n",
    "    \"ğŸ¤£\": \"Ù…Ø«Ø¨Øª\",  # positive\n",
    "    \"ğŸ˜Š\": \"Ø®ÙˆØ´ÛŒ\",  # happiness\n",
    "    \"ğŸ˜\": \"Ù…Ø«Ø¨Øª\",  # positive\n",
    "    \"ğŸ˜¡\": \"Ù…Ù†ÙÛŒ\",  # negative\n",
    "    \"ğŸ˜¢\": \"Ù…Ù†ÙÛŒ\",  # negative\n",
    "    \"ğŸ˜­\": \"Ù…Ù†ÙÛŒ\",  # negative\n",
    "    \"ğŸ‘\": \"Ù…Ø«Ø¨Øª\",  # positive\n",
    "    \"ğŸ‘\": \"Ù…Ù†ÙÛŒ\"   # negative\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "# Dictionary to map some common emojis to their sentiment words in Urdu\n",
    "emoji_sentiment_dict = {\n",
    "    \":face_with_tears_of_joy:\": \"Ù…Ø«Ø¨Øª\",  # ğŸ˜‚\n",
    "    \":rolling_on_the_floor_laughing:\": \"Ù…Ø«Ø¨Øª\",  # ğŸ¤£\n",
    "    \":smiling_face_with_smiling_eyes:\": \"Ø®ÙˆØ´ÛŒ\",  # ğŸ˜Š\n",
    "    \":smiling_face_with_sunglasses:\": \"Ù…Ø«Ø¨Øª\",  # ğŸ˜\n",
    "    \":angry_face:\": \"Ù…Ù†ÙÛŒ\",  # ğŸ˜¡\n",
    "    \":crying_face:\": \"Ù…Ù†ÙÛŒ\",  # ğŸ˜¢\n",
    "    \":loudly_crying_face:\": \"Ù…Ù†ÙÛŒ\",  # ğŸ˜­\n",
    "    \":thumbs_up:\": \"Ù…Ø«Ø¨Øª\",  # ğŸ‘\n",
    "    \":thumbs_down:\": \"Ù…Ù†ÙÛŒ\"   # ğŸ‘\n",
    "}\n",
    "\"\"\"\n",
    "def clean_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
    "    # Remove hashtags and mentions\n",
    "    text = re.sub(r'\\@\\w+|\\#\\w+', '', text)\n",
    "    # Remove punctuation (except for Urdu-specific punctuation)\n",
    "    text = re.sub(r'[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]', '', text)\n",
    "    # Replace emojis with sentiment words using the emoji library\n",
    "    #text = emoji.demojize(text)  # Convert emojis to text like :smiling_face_with_smiling_eyes:\n",
    "    for emoji_code, sentiment in emoji_sentiment_dict.items():\n",
    "        text = text.replace(emoji_code, sentiment)  # Replace emoji descriptions with sentiment words\n",
    "    text = emoji.replace_emoji(text, replace='')\n",
    "    return text\n",
    "\n",
    "# Apply the clean_text function to the 'urdu_text_no_stopwords' column\n",
    "df_cleansed['urdu_text_cleaned'] = df_cleansed['urdu_text_no_stopwords'].apply(clean_text)\n",
    "\n",
    "# Display before and after examples of text cleaning\n",
    "df_cleansed[['urdu_text_no_stopwords', 'urdu_text_cleaned']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864f9056-2f85-49f5-aaeb-5cf062a02e5b",
   "metadata": {},
   "source": [
    "## Removal of Short Posts less than 3 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69814dba-718a-42d4-a177-178117dd18b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 19102 entries, 0 to 20003\n",
      "Data columns (total 4 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   urdu_text               19102 non-null  object \n",
      " 1   is_sarcastic            19102 non-null  float64\n",
      " 2   urdu_text_no_stopwords  19102 non-null  object \n",
      " 3   urdu_text_cleaned       19102 non-null  object \n",
      "dtypes: float64(1), object(3)\n",
      "memory usage: 746.2+ KB\n"
     ]
    }
   ],
   "source": [
    "def filtered_short_posts(text):\n",
    "    word_count = len(text.split())\n",
    "    return word_count >= 3\n",
    "\n",
    "df_filtered = df_cleansed[df_cleansed['urdu_text_cleaned'].apply(filtered_short_posts)]\n",
    "df_filtered.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73c8c658-d78b-40b6-9f4a-6557e74b6ec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urdu_text_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ù…Ø«Ø¨ØªÙ…Ø«Ø¨ØªÙ…Ø«Ø¨Øª ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© Ú©...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ Ø¢Úº Ù…ÛŒÚº...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>`` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ø¨Ú¾ÛŒØ³ ÚˆÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ '' Ø­Ø§Ù…Ø¯ Ù…ÛŒØ±</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÛŒ Ø§Ú©Ø«Ø± Ù‚Ø§ØªÙ„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÙˆØªÛ’</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   urdu_text_cleaned\n",
       "0  Ù…Ø«Ø¨ØªÙ…Ø«Ø¨ØªÙ…Ø«Ø¨Øª ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© Ú©...\n",
       "1  Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ Ø¢Úº Ù…ÛŒÚº...\n",
       "2  Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...\n",
       "4    `` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ø¨Ú¾ÛŒØ³ ÚˆÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ '' Ø­Ø§Ù…Ø¯ Ù…ÛŒØ±\n",
       "5              Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÛŒ Ø§Ú©Ø«Ø± Ù‚Ø§ØªÙ„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÙˆØªÛ’ "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered[['urdu_text_cleaned']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edfc469-d96b-4df8-8386-380adb84227c",
   "metadata": {},
   "source": [
    "## Step: 2 Stemming And Lemmitization for Urdu Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d2fecf-2c34-40ec-8f5d-fd47a6e300a0",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96fad68c-1607-43e0-8892-23021cb899fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urdu_text_cleaned</th>\n",
       "      <th>urdu_text_stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>Ø±Ø§Ø¬Û ØµØ§Ø­Ø¨ ØªÙˆÚ‘ Ø³Ù†Ú¯ ØªÚ©Ø± Ú†Ú¾ÚˆÛŒØ§Û”Û”Û” ÛÙ† Ø¢ÙˆØ§Ø² Ù†Ø¦ÛŒ Ù†Ú©Ù„...</td>\n",
       "      <td>Ø±Ø§Ø¬Û ØµØ§Ø­Ø¨ ØªÙˆÚ‘ Ø³Ù†Ú¯ ØªÚ©Ø± Ú†Ú¾ÚˆÛŒØ§Û”Û”Û” ÛÙ† Ø¢ÙˆØ§Ø² Ù†Ø¦ÛŒ Ù†Ú©Ù„...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000</th>\n",
       "      <td>Ø¨Ø¹Ø¯ Ø¨Û’Ø¨ÛŒ Ù¾Ø±Ø§Ø¦Ù… Ù…Ù†Ø³Ù¹Ø± Ø¨Ù† Ú¯Ø¦ÛŒÛ”Û”Ù…Ø«Ø¨ØªÙ…Ø«Ø¨ØªÙ…Ø«Ø¨ØªÙ…Ø«Ø¨Øª</td>\n",
       "      <td>Ø¨Ø¹Ø¯ Ø¨Û’Ø¨ÛŒ Ù¾Ø±Ø§Ø¦Ù… Ù…Ù†Ø³Ù¹Ø± Ø¨Ù† Ú¯Ø¦ÛŒÛ”Û”Ù…Ø«Ø¨ØªÙ…Ø«Ø¨ØªÙ…Ø«Ø¨ØªÙ…Ø«Ø¨Øª</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20001</th>\n",
       "      <td>Ø§ØªÙ†Ø§ Ø¨ÙˆÙ†Ú¯Ø§ ÙˆØ²ÛŒØ± Ø§Ø¹Ø¸Ù… ÚˆÚ¾ÙˆÙ†ÚˆÙ†Û’ Ù…Ù„Û’ Ú¯Ø§</td>\n",
       "      <td>Ø§ØªÙ†Ø§ Ø¨ÙˆÙ†Ú¯Ø§ ÙˆØ²ÛŒØ± Ø§Ø¹Ø¸Ù… ÚˆÚ¾ÙˆÙ†ÚˆÙ†Û’ Ù…Ù„Û’ Ú¯</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20002</th>\n",
       "      <td>Ú©Ø§Ú©Ø§ ØªÙ… Ø¹ÙØ¯Øª Ù¾ÙˆØ±ÛŒ ÛÙˆÙ†Û’ Ø¯ÛŒ  Ø¹ÙˆØ§Ù… Ú©ÛŒØ³Û’ ØªÛŒØ±ÛŒ Ù…Ø¯Øª ...</td>\n",
       "      <td>Ú©Ø§Ú©Ø§ ØªÙ… Ø¹ÙØ¯Øª Ù¾ÙˆØ±ÛŒ ÛÙˆÙ†Û’ Ø¯ÛŒ Ø¹ÙˆØ§Ù… Ú©ÛŒØ³Û’ ØªÛŒØ±ÛŒ Ù…Ø¯Øª Ù¾...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20003</th>\n",
       "      <td>Ø¬ØªÙ†Ø§ Ù…Ø±Ø¶ÛŒ Ø¨Ù„ÛŒÚ© Ù…ÛŒÙ„ Ù„ÛŒÚº Ø§ÛŒÙ† Ø¢Ø± Ø§Ùˆ Ø¯ÙˆÚº Ú¯Ø§ØŒØ¬ØªÙ†Û’ Ù…...</td>\n",
       "      <td>Ø¬ØªÙ†Ø§ Ù…Ø±Ø¶ÛŒ Ø¨Ù„ÛŒÚ© Ù…ÛŒÙ„ Ù„ÛŒÚº Ø§ÛŒÙ† Ø¢Ø± Ø§Ùˆ Ø¯ÙˆÚº Ú¯Ø§ØŒØ¬ØªÙ†Û’ Ù…...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       urdu_text_cleaned  \\\n",
       "19999  Ø±Ø§Ø¬Û ØµØ§Ø­Ø¨ ØªÙˆÚ‘ Ø³Ù†Ú¯ ØªÚ©Ø± Ú†Ú¾ÚˆÛŒØ§Û”Û”Û” ÛÙ† Ø¢ÙˆØ§Ø² Ù†Ø¦ÛŒ Ù†Ú©Ù„...   \n",
       "20000      Ø¨Ø¹Ø¯ Ø¨Û’Ø¨ÛŒ Ù¾Ø±Ø§Ø¦Ù… Ù…Ù†Ø³Ù¹Ø± Ø¨Ù† Ú¯Ø¦ÛŒÛ”Û”Ù…Ø«Ø¨ØªÙ…Ø«Ø¨ØªÙ…Ø«Ø¨ØªÙ…Ø«Ø¨Øª   \n",
       "20001               Ø§ØªÙ†Ø§ Ø¨ÙˆÙ†Ú¯Ø§ ÙˆØ²ÛŒØ± Ø§Ø¹Ø¸Ù… ÚˆÚ¾ÙˆÙ†ÚˆÙ†Û’ Ù…Ù„Û’ Ú¯Ø§    \n",
       "20002  Ú©Ø§Ú©Ø§ ØªÙ… Ø¹ÙØ¯Øª Ù¾ÙˆØ±ÛŒ ÛÙˆÙ†Û’ Ø¯ÛŒ  Ø¹ÙˆØ§Ù… Ú©ÛŒØ³Û’ ØªÛŒØ±ÛŒ Ù…Ø¯Øª ...   \n",
       "20003  Ø¬ØªÙ†Ø§ Ù…Ø±Ø¶ÛŒ Ø¨Ù„ÛŒÚ© Ù…ÛŒÙ„ Ù„ÛŒÚº Ø§ÛŒÙ† Ø¢Ø± Ø§Ùˆ Ø¯ÙˆÚº Ú¯Ø§ØŒØ¬ØªÙ†Û’ Ù…...   \n",
       "\n",
       "                                       urdu_text_stemmed  \n",
       "19999  Ø±Ø§Ø¬Û ØµØ§Ø­Ø¨ ØªÙˆÚ‘ Ø³Ù†Ú¯ ØªÚ©Ø± Ú†Ú¾ÚˆÛŒØ§Û”Û”Û” ÛÙ† Ø¢ÙˆØ§Ø² Ù†Ø¦ÛŒ Ù†Ú©Ù„...  \n",
       "20000      Ø¨Ø¹Ø¯ Ø¨Û’Ø¨ÛŒ Ù¾Ø±Ø§Ø¦Ù… Ù…Ù†Ø³Ù¹Ø± Ø¨Ù† Ú¯Ø¦ÛŒÛ”Û”Ù…Ø«Ø¨ØªÙ…Ø«Ø¨ØªÙ…Ø«Ø¨ØªÙ…Ø«Ø¨Øª  \n",
       "20001                 Ø§ØªÙ†Ø§ Ø¨ÙˆÙ†Ú¯Ø§ ÙˆØ²ÛŒØ± Ø§Ø¹Ø¸Ù… ÚˆÚ¾ÙˆÙ†ÚˆÙ†Û’ Ù…Ù„Û’ Ú¯  \n",
       "20002  Ú©Ø§Ú©Ø§ ØªÙ… Ø¹ÙØ¯Øª Ù¾ÙˆØ±ÛŒ ÛÙˆÙ†Û’ Ø¯ÛŒ Ø¹ÙˆØ§Ù… Ú©ÛŒØ³Û’ ØªÛŒØ±ÛŒ Ù…Ø¯Øª Ù¾...  \n",
       "20003  Ø¬ØªÙ†Ø§ Ù…Ø±Ø¶ÛŒ Ø¨Ù„ÛŒÚ© Ù…ÛŒÙ„ Ù„ÛŒÚº Ø§ÛŒÙ† Ø¢Ø± Ø§Ùˆ Ø¯ÙˆÚº Ú¯Ø§ØŒØ¬ØªÙ†Û’ Ù…...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dictionary for stemming\n",
    "stemming_dict = {\n",
    "    'ÛÛ’': 'Û',\n",
    "    'ÛÛŒÚº': 'Û',\n",
    "    'ØªÚ¾Ø§': 'Û',\n",
    "    'ÛÙˆÚº': 'Û',\n",
    "    'ÛÙˆ': 'Û',\n",
    "    'Ú©Ø±': 'Ú©Ø±',\n",
    "    'Ú©ÛŒØ§': 'Ú©Ø±',\n",
    "    'Ú©Ø±ÛŒÚº': 'Ú©Ø±',\n",
    "    'Ú¯Ø§': 'Ú¯',\n",
    "    'Ú¯ÛŒØ§': 'Ú¯',\n",
    "    'Ø±ÛÛ’': 'Ø±Û',\n",
    "    'Ø±ÛØ§': 'Ø±Û',\n",
    "    'Ù¾Ø±': 'Ù¾Ø±',\n",
    "    'Ø³Û’': 'Ø³Û’',\n",
    "    'Ù…ÛŒÚº': 'Ù…ÛŒÚº',\n",
    "    'Ú©Ùˆ': 'Ú©Ùˆ',\n",
    "    'Ú©Û': 'Ú©Û',\n",
    "    'Ø¨Ú¾ÛŒ': 'Ø¨Ú¾ÛŒ',\n",
    "    'ØªÙˆ': 'ØªÙˆ',\n",
    "    'ÛÛŒ': 'Û',\n",
    "    'Ù†Û': 'Ù†Û',\n",
    "    'ÛŒØ§': 'ÛŒØ§',\n",
    "    # Add more mappings as needed\n",
    "}\n",
    "\n",
    "# A function for simple rule-based stemming using the stemming dictionary\n",
    "def urdu_stemmer(word):\n",
    "    return stemming_dict.get(word, word)  # Return the stemmed word if it exists, else return the word itself\n",
    "\n",
    "# Apply the stemmer to each word in the cleaned text\n",
    "def stem_text(text):\n",
    "    words = text.split()\n",
    "    stemmed_words = [urdu_stemmer(word) for word in words]\n",
    "    return ' '.join(stemmed_words)\n",
    "\n",
    "# Apply the stemming function to the 'urdu_text_cleaned' column\n",
    "df_filtered['urdu_text_stemmed'] = df_filtered['urdu_text_cleaned'].apply(stem_text)\n",
    "\n",
    "# Display before and after examples of stemming\n",
    "df_filtered[['urdu_text_cleaned', 'urdu_text_stemmed']].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d6d608d4-7b53-43e2-9baa-302d5a681607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urdu_text_cleaned</th>\n",
       "      <th>urdu_text_stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ù…Ø«Ø¨ØªÙ…Ø«Ø¨ØªÙ…Ø«Ø¨Øª ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© Ú©...</td>\n",
       "      <td>Ù…Ø«Ø¨ØªÙ…Ø«Ø¨ØªÙ…Ø«Ø¨Øª Û Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© Ú©Ùˆ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ Ø¢Úº Ù…ÛŒÚº...</td>\n",
       "      <td>Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ Ø¢Úº Ù…ÛŒÚº...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...</td>\n",
       "      <td>Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>`` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ø¨Ú¾ÛŒØ³ ÚˆÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ '' Ø­Ø§Ù…Ø¯ Ù…ÛŒØ±</td>\n",
       "      <td>`` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ø¨Ú¾ÛŒØ³ ÚˆÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ '' Ø­Ø§Ù…Ø¯ Ù…ÛŒØ±</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÛŒ Ø§Ú©Ø«Ø± Ù‚Ø§ØªÙ„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÙˆØªÛ’</td>\n",
       "      <td>Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªØ¨Ø§Ø± Û Ø§Ú©Ø«Ø± Ù‚Ø§ØªÙ„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÙˆØªÛ’</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   urdu_text_cleaned  \\\n",
       "0  Ù…Ø«Ø¨ØªÙ…Ø«Ø¨ØªÙ…Ø«Ø¨Øª ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© Ú©...   \n",
       "1  Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ Ø¢Úº Ù…ÛŒÚº...   \n",
       "2  Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...   \n",
       "4    `` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ø¨Ú¾ÛŒØ³ ÚˆÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ '' Ø­Ø§Ù…Ø¯ Ù…ÛŒØ±   \n",
       "5              Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÛŒ Ø§Ú©Ø«Ø± Ù‚Ø§ØªÙ„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÙˆØªÛ’    \n",
       "\n",
       "                                   urdu_text_stemmed  \n",
       "0  Ù…Ø«Ø¨ØªÙ…Ø«Ø¨ØªÙ…Ø«Ø¨Øª Û Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© Ú©Ùˆ...  \n",
       "1  Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ Ø¢Úº Ù…ÛŒÚº...  \n",
       "2  Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...  \n",
       "4    `` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ø¨Ú¾ÛŒØ³ ÚˆÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ '' Ø­Ø§Ù…Ø¯ Ù…ÛŒØ±  \n",
       "5                Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªØ¨Ø§Ø± Û Ø§Ú©Ø«Ø± Ù‚Ø§ØªÙ„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÙˆØªÛ’  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered[['urdu_text_cleaned', 'urdu_text_stemmed']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f91b6c-1a3a-4518-b41d-b1bad8ce6aa4",
   "metadata": {},
   "source": [
    "### Lemmitization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e83e48d8-84e3-4670-ac92-7ab4758d42ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urdu_text_stemmed</th>\n",
       "      <th>urdu_text_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ù…Ø«Ø¨ØªÙ…Ø«Ø¨ØªÙ…Ø«Ø¨Øª Û Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© Ú©Ùˆ...</td>\n",
       "      <td>Ù…Ø«Ø¨ØªÙ…Ø«Ø¨ØªÙ…Ø«Ø¨Øª Û Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© Ú©Ùˆ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ Ø¢Úº Ù…ÛŒÚº...</td>\n",
       "      <td>Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ Ø¢Úº Ù…ÛŒÚº...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...</td>\n",
       "      <td>Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§ Ø¬Ø§Ù†Ø§ Ø§Ù¾Ùˆ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>`` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ø¨Ú¾ÛŒØ³ ÚˆÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ '' Ø­Ø§Ù…Ø¯ Ù…ÛŒØ±</td>\n",
       "      <td>`` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ø¨Ú¾ÛŒØ³ ÚˆÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ '' Ø­Ø§Ù…Ø¯ Ù…ÛŒØ±</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªØ¨Ø§Ø± Û Ø§Ú©Ø«Ø± Ù‚Ø§ØªÙ„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÙˆØªÛ’</td>\n",
       "      <td>Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªØ¨Ø§Ø± Û Ø§Ú©Ø«Ø± Ù‚Ø§ØªÙ„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÙˆÙ†Ø§</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   urdu_text_stemmed  \\\n",
       "0  Ù…Ø«Ø¨ØªÙ…Ø«Ø¨ØªÙ…Ø«Ø¨Øª Û Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© Ú©Ùˆ...   \n",
       "1  Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ Ø¢Úº Ù…ÛŒÚº...   \n",
       "2  Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...   \n",
       "4    `` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ø¨Ú¾ÛŒØ³ ÚˆÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ '' Ø­Ø§Ù…Ø¯ Ù…ÛŒØ±   \n",
       "5                Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªØ¨Ø§Ø± Û Ø§Ú©Ø«Ø± Ù‚Ø§ØªÙ„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÙˆØªÛ’   \n",
       "\n",
       "                                urdu_text_lemmatized  \n",
       "0  Ù…Ø«Ø¨ØªÙ…Ø«Ø¨ØªÙ…Ø«Ø¨Øª Û Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© Ú©Ùˆ...  \n",
       "1  Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ Ø¢Úº Ù…ÛŒÚº...  \n",
       "2  Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§ Ø¬Ø§Ù†Ø§ Ø§Ù¾Ùˆ...  \n",
       "4    `` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ø¨Ú¾ÛŒØ³ ÚˆÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ '' Ø­Ø§Ù…Ø¯ Ù…ÛŒØ±  \n",
       "5                Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªØ¨Ø§Ø± Û Ø§Ú©Ø«Ø± Ù‚Ø§ØªÙ„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÙˆÙ†Ø§  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to apply lemmatization based on the dictionary\n",
    "def lemmatize_text(text, lemmatization_dict):\n",
    "    words = text.split()\n",
    "    # Replace words based on lemmatization dictionary\n",
    "    lemmatized_words = [lemmatization_dict.get(word, word) for word in words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "# Dictionary for lemmatization\n",
    "lemmatization_dict = {\n",
    "    'ÛŒÛ': 'ÛŒÛ',\n",
    "    'Ø¢Ù¾': 'Ø¢Ù¾',\n",
    "    'ÙˆÛ': 'ÙˆÛ',\n",
    "    'ÛÙ…': 'ÛÙ…',\n",
    "    'ØªÙ…': 'ØªÙˆ',\n",
    "    'Ø§Ù†': 'Ø§Ù†',\n",
    "    'Ú©Ø³ÛŒ': 'Ú©Ø³',\n",
    "    'Ø§Ù„Ù„Û': 'Ø§Ù„Ù„Û',\n",
    "    'Ø®Ø§Ù†': 'Ø®Ø§Ù†',\n",
    "    'Ø¨Ø§Øª': 'Ø¨Ø§Øª',\n",
    "    'ØµØ§Ø­Ø¨': 'ØµØ§Ø­Ø¨',\n",
    "    'Ù¾Ø§Ú©Ø³ØªØ§Ù†': 'Ù¾Ø§Ú©Ø³ØªØ§Ù†',\n",
    "    'Ø³Ù†Ø¯Ú¾': 'Ø³Ù†Ø¯Ú¾',\n",
    "    'Ø§Ø¨': 'Ø§Ø¨',\n",
    "    'Ø¨ÛØª': 'Ø¨Û',\n",
    "    'Ø³Ø¨': 'Ø³Ø¨',\n",
    "    'Ú©ÙˆØ¦ÛŒ': 'Ú©ÙˆØ¦ÛŒ',\n",
    "    'Ø§ÙˆØ±': 'Ø§ÙˆØ±',\n",
    "    'Ú©Ø§': 'Ú©',\n",
    "    'Ú©ÛŒ': 'Ú©',\n",
    "    'Ú©Û’': 'Ú©',\n",
    "    # New additions\n",
    "    'Ú¯Ø¦ÛŒ': 'Ø¬Ø§Ù†Ø§',\n",
    "    'Ù„Ú¯Ø§Ø¦ÛŒ': 'Ù„Ú¯Ø§',\n",
    "    'ÛÙˆØªÛ’': 'ÛÙˆÙ†Ø§',\n",
    "    'Ú©Ø±Ù†Ø§': 'Ú©Ø±',\n",
    "    'Ú©ÛØ§': 'Ú©ÛÙ†Ø§',\n",
    "    'Ù¾Ø§Ø¦ÛŒÙ†': 'Ù¾Ø§Ù†Ø§',\n",
    "    'Ø¬Ø§ØªÛŒ': 'Ø¬Ø§Ù†Ø§',\n",
    "    'Ø¨Ù†Ø§Ø¦ÛŒ': 'Ø¨Ù†Ù†Ø§'\n",
    "}\n",
    "\n",
    "# Apply the lemmatization function to the 'urdu_text_stemmed' column\n",
    "df_filtered['urdu_text_lemmatized'] = df_filtered['urdu_text_stemmed'].apply(lambda x: lemmatize_text(x, lemmatization_dict))\n",
    "\n",
    "# Display before and after examples of lemmatization\n",
    "df_filtered[['urdu_text_stemmed', 'urdu_text_lemmatized']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "00420df5-a705-4e84-a353-95e65b553190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urdu_text_stemmed</th>\n",
       "      <th>urdu_text_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>Ø±Ø§Ø¬Û ØµØ§Ø­Ø¨ ØªÙˆÚ‘ Ø³Ù†Ú¯ ØªÚ©Ø± Ú†Ú¾ÚˆÛŒØ§Û”Û”Û” ÛÙ† Ø¢ÙˆØ§Ø² Ù†Ø¦ÛŒ Ù†Ú©Ù„...</td>\n",
       "      <td>Ø±Ø§Ø¬Û ØµØ§Ø­Ø¨ ØªÙˆÚ‘ Ø³Ù†Ú¯ ØªÚ©Ø± Ú†Ú¾ÚˆÛŒØ§Û”Û”Û” ÛÙ† Ø¢ÙˆØ§Ø² Ù†Ø¦ÛŒ Ù†Ú©Ù„...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000</th>\n",
       "      <td>Ø¨Ø¹Ø¯ Ø¨Û’Ø¨ÛŒ Ù¾Ø±Ø§Ø¦Ù… Ù…Ù†Ø³Ù¹Ø± Ø¨Ù† Ú¯Ø¦ÛŒÛ”Û”Ù…Ø«Ø¨ØªÙ…Ø«Ø¨ØªÙ…Ø«Ø¨ØªÙ…Ø«Ø¨Øª</td>\n",
       "      <td>Ø¨Ø¹Ø¯ Ø¨Û’Ø¨ÛŒ Ù¾Ø±Ø§Ø¦Ù… Ù…Ù†Ø³Ù¹Ø± Ø¨Ù† Ú¯Ø¦ÛŒÛ”Û”Ù…Ø«Ø¨ØªÙ…Ø«Ø¨ØªÙ…Ø«Ø¨ØªÙ…Ø«Ø¨Øª</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20001</th>\n",
       "      <td>Ø§ØªÙ†Ø§ Ø¨ÙˆÙ†Ú¯Ø§ ÙˆØ²ÛŒØ± Ø§Ø¹Ø¸Ù… ÚˆÚ¾ÙˆÙ†ÚˆÙ†Û’ Ù…Ù„Û’ Ú¯</td>\n",
       "      <td>Ø§ØªÙ†Ø§ Ø¨ÙˆÙ†Ú¯Ø§ ÙˆØ²ÛŒØ± Ø§Ø¹Ø¸Ù… ÚˆÚ¾ÙˆÙ†ÚˆÙ†Û’ Ù…Ù„Û’ Ú¯</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20002</th>\n",
       "      <td>Ú©Ø§Ú©Ø§ ØªÙ… Ø¹ÙØ¯Øª Ù¾ÙˆØ±ÛŒ ÛÙˆÙ†Û’ Ø¯ÛŒ Ø¹ÙˆØ§Ù… Ú©ÛŒØ³Û’ ØªÛŒØ±ÛŒ Ù…Ø¯Øª Ù¾...</td>\n",
       "      <td>Ú©Ø§Ú©Ø§ ØªÙˆ Ø¹ÙØ¯Øª Ù¾ÙˆØ±ÛŒ ÛÙˆÙ†Û’ Ø¯ÛŒ Ø¹ÙˆØ§Ù… Ú©ÛŒØ³Û’ ØªÛŒØ±ÛŒ Ù…Ø¯Øª Ù¾...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20003</th>\n",
       "      <td>Ø¬ØªÙ†Ø§ Ù…Ø±Ø¶ÛŒ Ø¨Ù„ÛŒÚ© Ù…ÛŒÙ„ Ù„ÛŒÚº Ø§ÛŒÙ† Ø¢Ø± Ø§Ùˆ Ø¯ÙˆÚº Ú¯Ø§ØŒØ¬ØªÙ†Û’ Ù…...</td>\n",
       "      <td>Ø¬ØªÙ†Ø§ Ù…Ø±Ø¶ÛŒ Ø¨Ù„ÛŒÚ© Ù…ÛŒÙ„ Ù„ÛŒÚº Ø§ÛŒÙ† Ø¢Ø± Ø§Ùˆ Ø¯ÙˆÚº Ú¯Ø§ØŒØ¬ØªÙ†Û’ Ù…...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       urdu_text_stemmed  \\\n",
       "19999  Ø±Ø§Ø¬Û ØµØ§Ø­Ø¨ ØªÙˆÚ‘ Ø³Ù†Ú¯ ØªÚ©Ø± Ú†Ú¾ÚˆÛŒØ§Û”Û”Û” ÛÙ† Ø¢ÙˆØ§Ø² Ù†Ø¦ÛŒ Ù†Ú©Ù„...   \n",
       "20000      Ø¨Ø¹Ø¯ Ø¨Û’Ø¨ÛŒ Ù¾Ø±Ø§Ø¦Ù… Ù…Ù†Ø³Ù¹Ø± Ø¨Ù† Ú¯Ø¦ÛŒÛ”Û”Ù…Ø«Ø¨ØªÙ…Ø«Ø¨ØªÙ…Ø«Ø¨ØªÙ…Ø«Ø¨Øª   \n",
       "20001                 Ø§ØªÙ†Ø§ Ø¨ÙˆÙ†Ú¯Ø§ ÙˆØ²ÛŒØ± Ø§Ø¹Ø¸Ù… ÚˆÚ¾ÙˆÙ†ÚˆÙ†Û’ Ù…Ù„Û’ Ú¯   \n",
       "20002  Ú©Ø§Ú©Ø§ ØªÙ… Ø¹ÙØ¯Øª Ù¾ÙˆØ±ÛŒ ÛÙˆÙ†Û’ Ø¯ÛŒ Ø¹ÙˆØ§Ù… Ú©ÛŒØ³Û’ ØªÛŒØ±ÛŒ Ù…Ø¯Øª Ù¾...   \n",
       "20003  Ø¬ØªÙ†Ø§ Ù…Ø±Ø¶ÛŒ Ø¨Ù„ÛŒÚ© Ù…ÛŒÙ„ Ù„ÛŒÚº Ø§ÛŒÙ† Ø¢Ø± Ø§Ùˆ Ø¯ÙˆÚº Ú¯Ø§ØŒØ¬ØªÙ†Û’ Ù…...   \n",
       "\n",
       "                                    urdu_text_lemmatized  \n",
       "19999  Ø±Ø§Ø¬Û ØµØ§Ø­Ø¨ ØªÙˆÚ‘ Ø³Ù†Ú¯ ØªÚ©Ø± Ú†Ú¾ÚˆÛŒØ§Û”Û”Û” ÛÙ† Ø¢ÙˆØ§Ø² Ù†Ø¦ÛŒ Ù†Ú©Ù„...  \n",
       "20000      Ø¨Ø¹Ø¯ Ø¨Û’Ø¨ÛŒ Ù¾Ø±Ø§Ø¦Ù… Ù…Ù†Ø³Ù¹Ø± Ø¨Ù† Ú¯Ø¦ÛŒÛ”Û”Ù…Ø«Ø¨ØªÙ…Ø«Ø¨ØªÙ…Ø«Ø¨ØªÙ…Ø«Ø¨Øª  \n",
       "20001                 Ø§ØªÙ†Ø§ Ø¨ÙˆÙ†Ú¯Ø§ ÙˆØ²ÛŒØ± Ø§Ø¹Ø¸Ù… ÚˆÚ¾ÙˆÙ†ÚˆÙ†Û’ Ù…Ù„Û’ Ú¯  \n",
       "20002  Ú©Ø§Ú©Ø§ ØªÙˆ Ø¹ÙØ¯Øª Ù¾ÙˆØ±ÛŒ ÛÙˆÙ†Û’ Ø¯ÛŒ Ø¹ÙˆØ§Ù… Ú©ÛŒØ³Û’ ØªÛŒØ±ÛŒ Ù…Ø¯Øª Ù¾...  \n",
       "20003  Ø¬ØªÙ†Ø§ Ù…Ø±Ø¶ÛŒ Ø¨Ù„ÛŒÚ© Ù…ÛŒÙ„ Ù„ÛŒÚº Ø§ÛŒÙ† Ø¢Ø± Ø§Ùˆ Ø¯ÙˆÚº Ú¯Ø§ØŒØ¬ØªÙ†Û’ Ù…...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered[['urdu_text_stemmed', 'urdu_text_lemmatized']].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb70fad9-8343-4549-b029-261a8885f738",
   "metadata": {},
   "source": [
    "## Feature Extraction from Text "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313d6b02-97e3-4d73-b1da-46cfd867d561",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc363f00-95d8-45a7-a176-72e68124d0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hatee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4d4cacc9-d82c-4e16-ba15-5d3ba83d4b5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urdu_text_lemmatized</th>\n",
       "      <th>urdu_text_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ù…Ø«Ø¨ØªÙ…Ø«Ø¨ØªÙ…Ø«Ø¨Øª Û Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© Ú©Ùˆ...</td>\n",
       "      <td>[Ù…Ø«Ø¨ØªÙ…Ø«Ø¨ØªÙ…Ø«Ø¨Øª, Û, Ù„ÛŒÙ†Û’, Ø¯Û’, Ù…ÛŒØ±ÛŒ, Ø´Ø§Ø¯ÛŒ, ÙØ³Ø§Ø¯Ù†,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ Ø¢Úº Ù…ÛŒÚº...</td>\n",
       "      <td>[Ú†Ù„, Ù…ÛÙ…Ø§Ù†ÙˆÚº, Ú©Ú¾Ø§Ù†Ø§, Ø³Ø±Ùˆ, Ú†Ú‘ÛŒÙ„, Ú†Ø§Ú†ÛŒ, Ù†ÙˆÚº, Ø¯Ø³Ø¯...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§ Ø¬Ø§Ù†Ø§ Ø§Ù¾Ùˆ...</td>\n",
       "      <td>[Ú©Ø§Ù…Ø±Ø§Ù†, Ø®Ø§Ù†, Ø¢Ù¾Ú©ÛŒ, Ø¯Ù†, Ø¨Ú¾Ø±ÛŒÛ, Ø²Ù…Û, Ø¯Ø§Ø±ÛŒ, Ù„Ú¯Ø§,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>`` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ø¨Ú¾ÛŒØ³ ÚˆÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ '' Ø­Ø§Ù…Ø¯ Ù…ÛŒØ±</td>\n",
       "      <td>[``, Ù…Ø±Ø§Ø¯, Ø¹Ù„ÛŒ, Ø´Ø§Û, Ø¨Ú¾ÛŒØ³, ÚˆÛŒ, Ø¢Ø¦ÛŒ, Ø§ÛŒØ³, Ø¢Ø¦ÛŒ, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªØ¨Ø§Ø± Û Ø§Ú©Ø«Ø± Ù‚Ø§ØªÙ„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÙˆÙ†Ø§</td>\n",
       "      <td>[Ù‚Ø§Ø¨Ù„, Ø§Ø¹ØªØ¨Ø§Ø±, Û, Ø§Ú©Ø«Ø±, Ù‚Ø§ØªÙ„, Ø§Ø¹ØªØ¨Ø§Ø±, ÛÙˆÙ†Ø§]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                urdu_text_lemmatized  \\\n",
       "0  Ù…Ø«Ø¨ØªÙ…Ø«Ø¨ØªÙ…Ø«Ø¨Øª Û Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© Ú©Ùˆ...   \n",
       "1  Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ Ø¢Úº Ù…ÛŒÚº...   \n",
       "2  Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§ Ø¬Ø§Ù†Ø§ Ø§Ù¾Ùˆ...   \n",
       "4    `` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ø¨Ú¾ÛŒØ³ ÚˆÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ '' Ø­Ø§Ù…Ø¯ Ù…ÛŒØ±   \n",
       "5                Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªØ¨Ø§Ø± Û Ø§Ú©Ø«Ø± Ù‚Ø§ØªÙ„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÙˆÙ†Ø§   \n",
       "\n",
       "                                    urdu_text_tokens  \n",
       "0  [Ù…Ø«Ø¨ØªÙ…Ø«Ø¨ØªÙ…Ø«Ø¨Øª, Û, Ù„ÛŒÙ†Û’, Ø¯Û’, Ù…ÛŒØ±ÛŒ, Ø´Ø§Ø¯ÛŒ, ÙØ³Ø§Ø¯Ù†,...  \n",
       "1  [Ú†Ù„, Ù…ÛÙ…Ø§Ù†ÙˆÚº, Ú©Ú¾Ø§Ù†Ø§, Ø³Ø±Ùˆ, Ú†Ú‘ÛŒÙ„, Ú†Ø§Ú†ÛŒ, Ù†ÙˆÚº, Ø¯Ø³Ø¯...  \n",
       "2  [Ú©Ø§Ù…Ø±Ø§Ù†, Ø®Ø§Ù†, Ø¢Ù¾Ú©ÛŒ, Ø¯Ù†, Ø¨Ú¾Ø±ÛŒÛ, Ø²Ù…Û, Ø¯Ø§Ø±ÛŒ, Ù„Ú¯Ø§,...  \n",
       "4  [``, Ù…Ø±Ø§Ø¯, Ø¹Ù„ÛŒ, Ø´Ø§Û, Ø¨Ú¾ÛŒØ³, ÚˆÛŒ, Ø¢Ø¦ÛŒ, Ø§ÛŒØ³, Ø¢Ø¦ÛŒ, ...  \n",
       "5        [Ù‚Ø§Ø¨Ù„, Ø§Ø¹ØªØ¨Ø§Ø±, Û, Ø§Ú©Ø«Ø±, Ù‚Ø§ØªÙ„, Ø§Ø¹ØªØ¨Ø§Ø±, ÛÙˆÙ†Ø§]  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization function using NLTK\n",
    "def tokenize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "# Apply tokenization to the 'urdu_text_lemmatized' column\n",
    "df_filtered['urdu_text_tokens'] = df_filtered['urdu_text_lemmatized'].apply(tokenize_text)\n",
    "\n",
    "# Display tokenized examples\n",
    "df_filtered[['urdu_text_lemmatized', 'urdu_text_tokens']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c129f4-3023-40be-8c40-9ab44946623f",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fd37b616-6a9a-402e-9a97-9fe48752cbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4a784538-1815-48b1-972f-1ec61f7eddbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13209</th>\n",
       "      <td>Ù…Ø«Ø¨Øª</td>\n",
       "      <td>375.688422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19010</th>\n",
       "      <td>Ú©Ø±</td>\n",
       "      <td>365.657218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8797</th>\n",
       "      <td>Ø±Û</td>\n",
       "      <td>244.686340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21746</th>\n",
       "      <td>ÛÛ’</td>\n",
       "      <td>208.660022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13229</th>\n",
       "      <td>Ù…Ø«Ø¨ØªÙ…Ø«Ø¨Øª</td>\n",
       "      <td>196.127390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2538</th>\n",
       "      <td>Ø§Ù„Ù„Û</td>\n",
       "      <td>175.969492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19583</th>\n",
       "      <td>Ú©ÙˆØ¦ÛŒ</td>\n",
       "      <td>173.319169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19322</th>\n",
       "      <td>Ú©Ø³</td>\n",
       "      <td>171.979700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7141</th>\n",
       "      <td>Ø®Ø§Ù†</td>\n",
       "      <td>168.910178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13240</th>\n",
       "      <td>Ù…Ø«Ø¨ØªÙ…Ø«Ø¨ØªÙ…Ø«Ø¨Øª</td>\n",
       "      <td>165.919170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               word       score\n",
       "13209          Ù…Ø«Ø¨Øª  375.688422\n",
       "19010            Ú©Ø±  365.657218\n",
       "8797             Ø±Û  244.686340\n",
       "21746            ÛÛ’  208.660022\n",
       "13229      Ù…Ø«Ø¨ØªÙ…Ø«Ø¨Øª  196.127390\n",
       "2538           Ø§Ù„Ù„Û  175.969492\n",
       "19583          Ú©ÙˆØ¦ÛŒ  173.319169\n",
       "19322            Ú©Ø³  171.979700\n",
       "7141            Ø®Ø§Ù†  168.910178\n",
       "13240  Ù…Ø«Ø¨ØªÙ…Ø«Ø¨ØªÙ…Ø«Ø¨Øª  165.919170"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the TfidfVectorizer with a reduced feature set\n",
    "tfidf_vectorizer_reduced = TfidfVectorizer()\n",
    "\n",
    "# Combine the tokenized words back into sentences for the TF-IDF vectorizer\n",
    "df_filtered['urdu_text_for_tfidf'] = df_filtered['urdu_text_tokens'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Fit and transform the text data to compute TF-IDF scores with reduced features\n",
    "tfidf_matrix_reduced = tfidf_vectorizer_reduced.fit_transform(df_filtered['urdu_text_for_tfidf'])\n",
    "\n",
    "# Get feature names (words) and their respective TF-IDF scores\n",
    "tfidf_feature_names_reduced = tfidf_vectorizer_reduced.get_feature_names_out()\n",
    "tfidf_scores_reduced = tfidf_matrix_reduced.toarray()\n",
    "\n",
    "# Sum TF-IDF scores across all documents for each feature (word)\n",
    "word_scores_reduced = tfidf_scores_reduced.sum(axis=0)\n",
    "\n",
    "# Create a DataFrame to sort and display top terms by their TF-IDF scores\n",
    "tfidf_df_reduced = pd.DataFrame({'word': tfidf_feature_names_reduced, 'score': word_scores_reduced})\n",
    "tfidf_top_words_reduced = tfidf_df_reduced.sort_values(by='score', ascending=False).head(10)\n",
    "\n",
    "# Display the top 10 words with highest TF-IDF scores\n",
    "tfidf_top_words_reduced\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e7c568-1d68-4be4-a50c-531d2a9f5d87",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3ef954c2-208c-4e17-bc06-9d52df802942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\hatee\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\hatee\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\hatee\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\hatee\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gensim) (7.0.4)\n",
      "Requirement already satisfied: wrapt in c:\\users\\hatee\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.14.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "807857ee-cb07-4360-821b-e1447992cc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade scipy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a01bf955-6bd2-4d68-9e3c-bcdd98f01be8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ø¨Ù„Ø§Ú©', 0.9905962347984314),\n",
       " ('Ø³ÙˆØ§Ø±', 0.9902935028076172),\n",
       " ('Ø¯Ø±Ø¯', 0.9901655912399292),\n",
       " ('Ø¨ÛØªØ±', 0.9896510243415833),\n",
       " ('Ù…Ø«Ø¨ØªÙ…Ø«Ø¨ØªÙ…Ø«Ø¨ØªÙ…Ø«Ø¨ØªÙ…Ø«Ø¨ØªÙ…Ø«Ø¨ØªÙ…Ø«Ø¨Øª', 0.9896172881126404)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from gensim.models import Word2Vec \n",
    "\n",
    "# Apply the lemmatization function to the 'urdu_text_stemmed' column\n",
    "df_filtered['urdu_text_lemmatized'] = df_filtered['urdu_text_stemmed'].apply(lambda x: lemmatize_text(x, lemmatization_dict))\n",
    "\n",
    "# Tokenization function using NLTK\n",
    "def tokenize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "# Apply tokenization to the 'urdu_text_lemmatized' column\n",
    "df_filtered['urdu_text_tokens'] = df_filtered['urdu_text_lemmatized'].apply(tokenize_text)\n",
    "\n",
    "# Prepare the tokenized text data for Word2Vec\n",
    "tokenized_sentences = df_filtered['urdu_text_tokens'].tolist()\n",
    "\n",
    "# Train the Word2Vec model\n",
    "w2v_model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=2, workers=4, sg=0)\n",
    "\n",
    "# Find the top 5 words most similar to \"Ø§Ú†Ú¾Ø§\" (good)\n",
    "try:\n",
    "    similar_words = w2v_model.wv.most_similar(\"Ø§Ú†Ú¾Ø§\", topn=5)\n",
    "except KeyError:\n",
    "    similar_words = \"The word 'Ø§Ú†Ú¾Ø§' was not found in the vocabulary.\"\n",
    "\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d512db",
   "metadata": {},
   "source": [
    "### N-GRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "26d87000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(('Ø¹Ù…Ø±Ø§Ù†', 'Ø®Ø§Ù†'), 494),\n",
       "  (('Ù†ÙˆØ§Ø²', 'Ø´Ø±ÛŒÙ'), 442),\n",
       "  (('Ø³Ù†Ø¯Ú¾', 'Ù¾ÙˆÙ„ÛŒØ³'), 299),\n",
       "  (('Û', 'Ú¯'), 260),\n",
       "  (('Ø¢Ø±Ù…ÛŒ', 'Ú†ÛŒÙ'), 223),\n",
       "  (('Ø®Ø§Ù†', 'ØµØ§Ø­Ø¨'), 179),\n",
       "  (('Ú©ÛŒÙ¾Ù¹Ù†', 'ØµÙØ¯Ø±'), 177),\n",
       "  (('Ù…Ø«Ø¨Øª', 'Ù…Ø«Ø¨Øª'), 166),\n",
       "  (('Ù…Ø±ÛŒÙ…', 'Ù†ÙˆØ§Ø²'), 157),\n",
       "  (('Ø¬Ø²Ø§Ú©', 'Ø§Ù„Ù„Û'), 156)],\n",
       " [(('Ù¾ÛŒ', 'Ù¹ÛŒ', 'Ø¢Ø¦ÛŒ'), 114),\n",
       "  (('ØµÙ„ÛŒ', 'Ø§Ù„Ù„Û', 'Ø¹Ù„ÛŒÛ'), 87),\n",
       "  (('Ù¾ÛŒ', 'ÚˆÛŒ', 'Ø§ÛŒÙ…'), 86),\n",
       "  (('Ù…Ø«Ø¨Øª', 'Ù…Ø«Ø¨Øª', 'Ù…Ø«Ø¨Øª'), 85),\n",
       "  (('Ø¬Ø²Ø§Ú©', 'Ø§Ù„Ù„Û', 'Ø®ÛŒØ±'), 70),\n",
       "  (('ÙØ§Ù„Ùˆ', 'Ú©Ø±', 'ÙØ§Ù„Ùˆ'), 69),\n",
       "  (('Ú©Ø±', 'ÙØ§Ù„Ùˆ', 'Ø¨ÛŒÚ©'), 63),\n",
       "  (('ÙˆØ§Ù„ÙˆÚº', 'ÙØ§Ù„Ùˆ', 'Ú©Ø±'), 60),\n",
       "  (('Ø§ÛŒØ³', 'Ø§ÛŒÚ†', 'Ø§Ùˆ'), 55),\n",
       "  (('ÙØ§Ù„ÙˆØ±Ø²', 'Ø§Ø¶Ø§ÙÛ', 'Ú©Ø±'), 53)])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "# Function to generate n-grams and calculate their frequencies\n",
    "def generate_ngrams(tokens, n):\n",
    "    return list(ngrams(tokens, n))\n",
    "\n",
    "# Collect all tokens for n-grams analysis\n",
    "all_tokens = df_filtered['urdu_text_tokens'].tolist()\n",
    "\n",
    "# Flatten the list of token lists into a single list of tokens\n",
    "flattened_tokens = [token for tokens in all_tokens for token in tokens]\n",
    "\n",
    "# Generate bigrams and trigrams\n",
    "bigrams = generate_ngrams(flattened_tokens, 2)\n",
    "trigrams = generate_ngrams(flattened_tokens, 3)\n",
    "\n",
    "# Calculate frequencies of bigrams and trigrams\n",
    "bigram_freq = Counter(bigrams)\n",
    "trigram_freq = Counter(trigrams)\n",
    "\n",
    "# Get the top 10 most common bigrams and trigrams\n",
    "top_10_bigrams = bigram_freq.most_common(10)\n",
    "top_10_trigrams = trigram_freq.most_common(10)\n",
    "\n",
    "# Display the top 10 bigrams and trigrams along with their counts\n",
    "top_10_bigrams, top_10_trigrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "78bf969b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.75      0.75      1848\n",
      "         1.0       0.76      0.76      0.76      1973\n",
      "\n",
      "    accuracy                           0.76      3821\n",
      "   macro avg       0.76      0.76      0.76      3821\n",
      "weighted avg       0.76      0.76      0.76      3821\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Step 1: Prepare Data for Modeling\n",
    "# Use TF-IDF features\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df_filtered['urdu_text_lemmatized'])\n",
    "\n",
    "# Target variable\n",
    "y = df_filtered['is_sarcastic']\n",
    "\n",
    "# Step 2: Split Data into Training and Test Sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Train the Model\n",
    "# Use Logistic Regression as the classifier\n",
    "model = LogisticRegression(max_iter=200)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 4: Evaluate the Model\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy, precision, recall, and F1-score\n",
    "classification_report_result = classification_report(y_test, y_pred)\n",
    "confusion_matrix_result = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(classification_report_result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
